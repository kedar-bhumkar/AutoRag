Source URL: https://github.com/Chen-zexi/open-ptc-agent
Title: GitHub - Chen-zexi/open-ptc-agent: An open source implementation of code execution with MCP (Programatic Tool Calling)

Chen-zexi / open-ptc-agent Public Notifications You must be signed in to change notification settings Fork 66 Star 560 An open source implementation of code execution with MCP (Programatic Tool Calling) License MIT license 560 stars 66 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 1 Pull requests 0 Actions Projects 0 Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights Chen-zexi/open-ptc-agent main Branches Tags Go to file Code Open more actions menu Folders and files Name Name Last commit message Last commit date Latest commit History 16 Commits docs docs example example mcp_servers mcp_servers src src tests tests .env.example .env.example .gitattributes .gitattributes .gitignore .gitignore LICENSE LICENSE PTC_Agent.ipynb PTC_Agent.ipynb README.md README.md README_zh.md README_zh.md config.yaml config.yaml langgraph.json langgraph.json llms.json llms.json pyproject.toml pyproject.toml utils.py utils.py uv.lock uv.lock View all files Repository files navigation README MIT license Open PTC Agent English | 中文 Getting Started | Demo Notebooks | Configuration | Changelog | Roadmap What is Programmatic Tool Calling? This project is an open source implementation of Anthropic recently introduced Programmatic Tool Calling (PTC) , which enables agents to invoke tools with code execution rather than making individual JSON tool calls. This paradigm is also featured in their earlier engineering blog Code execution with MCP . Why PTC? LLMs are exceptionally good at writing code! They excel at understanding context, reasoning about data flows, and generating precise logic. PTC lets them do what they do best - write code that orchestrates entire workflows rather than reasoning through one tool call at a time. Traditional tool calling returns full results to the model's context window. Suppose fetching 1 year of daily stock prices for 10 tickers. This means 2,500+ OHLCV data points polluting context - tens of thousands of tokens just to compute a portfolio summary. With PTC, code runs in a sandbox, processes data locally, and only the final output returns to the model. Result: 85-98% token reduction. PTC particularly shines when working with large volumes of structured data, time series data (like financial market data), and scenarios requiring further data processing - filtering, aggregating, transforming, or visualizing results before returning them to the model. How It Works This project is implementing based on deep-agent from langchain-ai and daytona for sandbox environment. User Task
    |
    v
+-------------------+
|    PTCAgent       |  Tool discovery -> Writes Python code
+-------------------+
    |       ^
    v       |
+-------------------+
|  Daytona Sandbox  |  Executes code
|  +-------------+  |
|  | MCP Tools   |  |  tool() -> process / filter / aggregate -> dump to data/ directory
|  | (Python)    |  |
|  +-------------+  |
+-------------------+
    |
    v
+-------------------+
|Final deliverables |  Files and data can be downloaded from sandbox
+-------------------+ What's New Background Subagent Execution - Subagents now run asynchronously using a "fire and collect" pattern, giving the agent proactive control over dispatched tasks. Results are automatically returned to the agent upon completion, even without explicitly calling wait() Vision/Multimodal Support - New view_image tool enables vision-capable LLMs to analyze images from URLs, base64 data, or sandbox files Task Monitoring - New wait() and task_progress() tools for monitoring and collecting background task results Features Universal MCP Support - Auto-converts any MCP server tools to Python functions Progressive Tool Discovery - Tools discovered on-demand; avoids large number of tokens of upfront tool definitions Custom MCP Upload - Deploy Python MCP implementations directly into sandbox sessions Enhanced File Tools - Refined glob, grep and other file operation tools based on LangChain DeepAgent Daytona Backend - Secure code execution with filesystem isolation and snapshot support Auto Image Upload - Charts and images auto-uploaded to cloud storage (Cloudflare R2, AWS S3, Alibaba OSS) LangGraph Ready - Compatible with LangGraph Cloud/Studio deployment Multi-LLM Support - Works with Anthropic, OpenAI, and Any LLM provider you configure in llms.json Project Structure ├── src/
│   ├── ptc_core/              # Core infrastructure
│   │   ├── sandbox.py         # PTCSandbox (glob, grep, read, write)
│   │   ├── mcp_registry.py    # MCP server discovery & connection
│   │   ├── tool_generator.py  # MCP schema → Python functions
│   │   ├── session.py         # Session lifecycle management
│   │   └── config.py          # Configuration classes
│   │
│   └── agent/                 # Agent implementation
│       ├── agent.py           # PTCAgent, PTCExecutor
│       ├── config.py          # AgentConfig
│       ├── tools/             # Native tool implementations
│       ├── prompts/           # Jinja2 templates
│       ├── subagents/         # Research & general-purpose subagents
│       ├── middleware/        # Background execution, vision support
│       └── backends/          # DaytonaBackend
│
├── mcp_servers/               # Custom MCP server implementations for demo purposes
│   ├── yfinance_mcp_server.py
│   └── tickertick_mcp_server.py
│
├── config.yaml                # Main configuration
├── llms.json                  # LLM provider definitions
└── PTC_Agent.ipynb            # Demo notebook Native Tools The agent has access to native tools plus middleware capabilities from deep-agent : Core Tools Tool Description Key Parameters execute_code Execute Python with MCP tool access code Bash Run shell commands command , timeout , working_dir Read Read file with line numbers file_path , offset , limit Write Write/overwrite file file_path , content Edit Exact string replacement file_path , old_string , new_string Glob File pattern matching pattern , path Grep Content search (ripgrep) pattern , path , output_mode Middleware (via langchain/deep-agent) Middleware Description Tools Provided SubagentsMiddleware Delegates specialized tasks to sub-agents with isolated execution task() BackgroundSubagentMiddleware Async subagent execution with fire and collect pattern wait() , task_progress() ViewImageMiddleware Injects images into conversation for multimodal LLMs view_image() FilesystemMiddleware File operations read_file , write_file , edit_file , glob , grep , ls TodoListMiddleware Task planning and progress tracking (auto-enabled) write_todos SummarizationMiddleware Auto-summarizes conversation history (auto-enabled) - Available Subagents: research - Web search with Tavily + think tool for strategic reflection general-purpose - Full execute_code, filesystem, and vision tools for complex multi-step tasks Subagents run in the background by default - the main agent can continue working while delegated tasks execute asynchronously. Note: For better tool discovery, I override the built-in filesystem middleware from langchain deep-agent. You can disable it by setting use_custom_filesystem_tools to false in config.yaml . MCP Integration Demo MCP Servers The demo includes 3 enabled MCP servers configured in config.yaml : Server Transport Tools Purpose tavily stdio (npx) 4 Web search yfinance stdio (python) 10 Stock prices, financials tickertick stdio (python) 7 Financial news How MCP Tools Appear In Prompts - Tool summaries are injected into the system prompt: tavily: Web search engine for finding current information
  - Module: tools/tavily.py
  - Tools: 4 tools available
  - Import: from tools.tavily import <tool_name> In Sandbox - Full Python modules are generated: /home/daytona/
├── tools/
│   ├── mcp_client.py      # MCP communication layer
│   ├── tavily.py          # from tools.tavily import search
│   ├── yfinance.py        # from tools.yfinance import get_stock_history
│   └── docs/              # Auto-generated documentation
│       ├── tavily/*.md
│       └── yfinance/*.md
├── results/               # Agent output
└── data/                  # Input data In Code - Agent imports and uses tools directly: from tools . yfinance import get_stock_history import pandas as pd # Fetch data - stays in sandbox history = get_stock_history ( ticker = "AAPL" , period = "1y" ) # Process locally - no tokens wasted df = pd . DataFrame ( history ) summary = { "mean" : df [ "close" ]. mean (), "volatility" : df [ "close" ]. std ()} # Only summary returns to model print ( summary ) Getting Started Prerequisites Python 3.12+ Node.js (for MCP servers) uv package manager Installation git clone https://github.com/Chen-zexi/open-ptc-agent.git cd open-ptc-agent
uv sync Minimal Configuration Create a .env file with the minimum required keys: # One LLM provider (choose one) ANTHROPIC_API_KEY=your-key # or OPENAI_API_KEY=your-key # or # Any model you configred in llms.json and config.yaml # Daytona (required) DAYTONA_API_KEY=your-key Get your Daytona API key from Daytona Dashboard . They provide free credits for new users! Extended Configuration For full functionality, add optional keys: # MCP Servers TAVILY_API_KEY=your-key # Web search ALPHA_VANTAGE_API_KEY=your-key # Financial data # Cloud Storage (choose one provider) R2_ACCESS_KEY_ID=... # Cloudflare R2 AWS_ACCESS_KEY_ID=... # AWS S3 OSS_ACCESS_KEY_ID=... # Alibaba OSS # Tracing (optional) LANGSMITH_API_KEY=your-key See .env.example for the complete list of configuration options. Demo Notebooks Quick start with the jupyter notebooks: PTC_Agent.ipynb - Quick demo with open-ptc-agent example/Subagent_demo.ipynb - Background subagent execution with research and general-purpose agents Optionally, you can use the langgraph api to deploy the agent. Configuration The project uses two configuration files: config.yaml - Main configuration (LLM selection, MCP servers, Daytona, security, storage) llms.json - LLM provider definitions Quick Config Select your LLM in config.yaml : llm : name : " claude-sonnet-4-5 " # Options: claude-sonnet-4-5, gpt-5.1-codex-mini, gemini-3-pro Enable/disable MCP servers: mcp : servers :
    - name : " tavily " enabled : true # Set to false to disable For complete configuration options including Daytona settings, security policies, and adding custom LLM providers, see the Configuration Guide . Roadmap Planned features and improvements: CI/CD pipeline for automated testing Additional MCP server integrations / More example notebooks Performance benchmarks and optimizations Improved search tool for smoother tool discovery Claude skill integration CLI Version for PTC Agent (Release soon) Contributing We welcome contributions from the community! Here are some ways you can help: Code Contributions - Bug fixes, new features, improvements (CI/CD coming soon) Use Cases - Share how you're using PTC in production or research Example Notebooks - Create demos showcasing different workflows MCP Servers - Build or recommend MCP servers that work well with PTC (data processing, APIs, etc.) Prompt Tricks - Share prompting techniques that improve agent performance Open an issue or PR on GitHub to contribute! Acknowledgements This project builds on research and tools from: Research/Articles Introducing advanced tool use on the Claude Developer Platform - Anthropic Code execution with MCP: building more efficient AI agents - Anthropic CodeAct: Executable Code Actions Elicit Better LLM Agents - Wang et al. Frameworks and Infrastructure LangChain DeepAgents - Base Agent Framework Daytona - Sandbox infrastructure Star History If you find this project useful, please consider giving it a star! It helps others discover this work. License MIT License About An open source implementation of code execution with MCP (Programatic Tool Calling) Topics agent mcp daytona llm langchain langraph Resources Readme License MIT license Uh oh! There was an error while loading. Please reload this page . Activity Stars 560 stars Watchers 13 watching Forks 66 forks Report repository Languages Python 96.6% Jinja 3.4%