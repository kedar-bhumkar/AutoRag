Source URL: https://www.reddit.com/r/AI_Agents/s/6F2bqlCnDP
Title: Reddit - The heart of the internet

Go to AI_Agents r/AI_Agents AI-builder-sf-accel Top LLM Evaluation Platforms: In Depth Comparison Discussion I’ve been testing the LLM Evaluation platforms in incredible depth over the last 12+ months. I’ve been leveraging a couple of these LLM evaluation and observability solutions to improve my own agent. I know everyone could use this advice so dropping a bit here. Agents work over sessions or tasks as they either interact with people, build code or accomplish work. We have found we just live in session level views of our data every day. We evaluate over sessions and our goal is to improve the outcome at the end of the session. We have found we session level analysis, session annotations, and session evaluations are key to improving agents. Arize Ax: One of the better Agent Evaluation, Observability solutions we tested. Ax supports a large set of Agent centric debugging workflows like agent session evaluations, session annotations, agent framework tracing, and agent graph visualization. Alyx is a “Cursor like” AI Agent for AI Engineers that helps you debug and build your AI agents - the best in the ecosystem. LangSmith : Built for LangChain and LangGraph users, LangSmith excels at tracing, debugging, and evaluating LangGraph workflows. It has deep integration with LangGraph and if teams are all in on the LangChain ecosystem it is a good integrated solution. It tends to be more proprietary than other solutions both in how it integrates with frameworks and instrumentation. Ecosystem lock-in is the risk with this one. Braintrust : Focused on prompt-first Evaluation, Braintrust enables fast prompt iteration, benchmarking, and dataset management. Braintrust is stronger in development and playground workflows but weaker in features needed for agent evaluation. Braintrust online evaluations are less useful for agents as they lack things like session level evaluations, agent session annotations and agent graph debugging workflows. Arize Phoenix Open Source: Open Source Agent Application Observability and Evaluation. Phoenix focuses on Observability (first to market with OTEL), Evaluation Online/Offline libraries, Prompt replay, Prompt playground and Evaluation Experiments. Strong OSS Evaluation solution with an entire Eval library in TS and Python. Phoenix offers a great option for teams who start with open source but want to upgrade to a solid enterprise solution in Arize Ax. We found it was pretty seamless. LangFuse Open Source : Open Source LLM Engineering platform. Popular open source solution for tracing your AI and agent applications. LangFuse is easy to get started with and has a wealth of features. LangFuse started in Observability & cost tracking and added Evaluation recently. Very strong tracing but weaker evaluation solution. LangFuse's biggest issue is the lack of enterprise deployment support, they are not a big enough company to support the larger companies. None of these is perfect and each has various trade offs. If you are building with agents and you want an independent player Arize Ax is probably the best. If you love the LangChain ecosystem, LangSmith is solid If you start with wanting your LLM Evaluations to be open source, and you care about agents & evaluations Arize Phoenix is a great option If you want a popular open source library that is solid at tracing LangFuse is a great option Hope this helps, would love to hear others thoughts: Read more Share Related Answers Section Related Answers Top LLM evaluation platforms comparison Best observability tools for LLMs Opik vs Langfuse comparison Langfuse vs Phoenix comparison Best cursor model for coding